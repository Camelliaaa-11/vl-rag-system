"""
å±•è§ˆæ•°æ®RAGå…¥åº“ä¸»ç¨‹åº
"""
import os
import sys
import glob
from pathlib import Path
from typing import List, Dict, Any, Tuple
import uuid
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))

from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# å¯¼å…¥æœ¬åœ°æ¨¡å—
# åœ¨ ingest.py ä¸­ä¿®æ”¹å¯¼å…¥éƒ¨åˆ†
try:
    from backend.rag.embeddings import get_embedding_manager
    # ä½¿ç”¨ä¿®æ”¹åçš„æ–°æ ¼å¼åŠ è½½å™¨
    from backend.rag.excel_loader import load_complex_exhibition_excel as load_exhibition_excel_files
    EXHIBITION_LOADER_AVAILABLE = True
    EMBEDDINGS_AVAILABLE = True  # æ·»åŠ è¿™è¡Œ
except ImportError as e:
    print(f"âš ï¸  æ¨¡å—å¯¼å…¥è­¦å‘Š: {e}")
    EXHIBITION_LOADER_AVAILABLE = False
    EMBEDDINGS_AVAILABLE = False  # æ·»åŠ è¿™è¡Œ

def load_documents(data_dir: str = "data/raw_docs") -> List[Document]:
    """
    åŠ è½½æ‰€æœ‰æ–‡æ¡£

    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„

    Returns:
        æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
    """
    documents = []

    print("=" * 70)
    print("ğŸ¨ è‰ºæœ¯ä¸ç§‘æŠ€å±•è§ˆæ•°æ®RAGå…¥åº“ç³»ç»Ÿ")
    print("=" * 70)
    print(f"ğŸ“ æ•°æ®ç›®å½•: {os.path.abspath(data_dir)}")

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        print(f"åˆ›å»ºç›®å½•: {data_dir}")
        os.makedirs(data_dir, exist_ok=True)
        print(f"âœ… ç›®å½•å·²åˆ›å»ºï¼Œè¯·å°†Excelæ–‡ä»¶æ”¾å…¥æ­¤ç›®å½•")
        return documents

    # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©º
    all_files = os.listdir(data_dir)
    if not all_files:
        print(f"âš ï¸  æ•°æ®ç›®å½•ä¸ºç©º")
        print(f"è¯·å°†Excelæ–‡ä»¶æ”¾å…¥: {data_dir}")
        print(f"æ–‡ä»¶è¦æ±‚:")
        print(f"  - æ ¼å¼: .xlsx æˆ– .xls")
        print(f"  - å†…å®¹: åŒ…å«å±•åŒºã€ä½œå“åç§°ã€æè¿°ç­‰ä¿¡æ¯")
        print(f"  - å»ºè®®: åŒ…å«å¤šä¸ªsheetï¼ˆå·¥ä¸šè®¾è®¡ã€ç¯å¢ƒè®¾è®¡ã€è‰ºæœ¯ä¸ç§‘æŠ€ï¼‰")
        return documents

    print(f"ğŸ“‹ å‘ç° {len(all_files)} ä¸ªæ–‡ä»¶")

    # 1. ä¼˜å…ˆåŠ è½½å±•è§ˆExcelæ•°æ®
    if EXHIBITION_LOADER_AVAILABLE:
        print(f"\nğŸ“Š å¤„ç†å±•è§ˆExcelæ•°æ®...")
        exhibition_docs = load_exhibition_excel_files(data_dir)

        if exhibition_docs:
            documents.extend(exhibition_docs)
            print(f"âœ… å±•è§ˆæ•°æ®: {len(exhibition_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°å±•è§ˆæ•°æ®ï¼Œå°è¯•å…¶ä»–æ ¼å¼...")
    else:
        print(f"âŒ å±•è§ˆåŠ è½½å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡Excelå¤„ç†")

    # 2. åŠ è½½å…¶ä»–æ ¼å¼æ–‡æ¡£ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not documents:
        print(f"\nğŸ“„ å°è¯•åŠ è½½å…¶ä»–æ ¼å¼æ–‡æ¡£...")
        other_docs = _load_other_formats(data_dir)

        if other_docs:
            documents.extend(other_docs)
            print(f"âœ… å…¶ä»–æ ¼å¼: {len(other_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°å…¶ä»–æ ¼å¼æ–‡æ¡£")

    # 3. å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºæµ‹è¯•æ•°æ®
    if not documents:
        print(f"\nğŸ§ª åˆ›å»ºæµ‹è¯•æ•°æ®...")
        test_docs = _create_test_documents()
        documents.extend(test_docs)
        print(f"âœ… æµ‹è¯•æ•°æ®: {len(test_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        print(f"âš ï¸  æ³¨æ„: è¿™æ˜¯æµ‹è¯•æ•°æ®ï¼Œè¯·æ”¾å…¥çœŸå®çš„Excelæ–‡ä»¶")

    # 4. åˆ›å»ºç³»ç»Ÿæ‘˜è¦æ–‡æ¡£
    if documents:
        print(f"\nğŸ“ åˆ›å»ºç³»ç»Ÿæ‘˜è¦...")
        summary_docs = _create_system_summary(documents, data_dir)
        documents.extend(summary_docs)
        print(f"âœ… ç³»ç»Ÿæ‘˜è¦: {len(summary_docs)} ä¸ªæ–‡æ¡£")

    print(f"\nğŸ“ˆ æ–‡æ¡£åŠ è½½ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£æ•°: {len(documents)}")

    # æ–‡æ¡£ç±»å‹ç»Ÿè®¡
    doc_types = {}
    categories = {}

    for doc in documents:
        doc_type = doc.metadata.get("type", "unknown")
        category = doc.metadata.get("category", "æœªçŸ¥")

        doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        categories[category] = categories.get(category, 0) + 1

    print(f"  æ–‡æ¡£ç±»å‹: {doc_types}")
    print(f"  ä½œå“ç±»åˆ«: {categories}")

    return documents

def _load_other_formats(data_dir: str) -> List[Document]:
    """
    åŠ è½½å…¶ä»–æ ¼å¼æ–‡æ¡£ï¼ˆPDFã€TXTã€MDï¼‰

    Args:
        data_dir: æ•°æ®ç›®å½•

    Returns:
        æ–‡æ¡£åˆ—è¡¨
    """
    documents = []

    try:
        from langchain_community.document_loaders import (
            DirectoryLoader, PyPDFLoader, TextLoader
        )

        # PDFæ–‡ä»¶
        pdf_pattern = os.path.join(data_dir, "**/*.pdf")
        if glob.glob(pdf_pattern, recursive=True):
            print(f"  ğŸ“• åŠ è½½PDFæ–‡ä»¶...")
            pdf_loader = DirectoryLoader(
                data_dir,
                glob="**/*.pdf",
                loader_cls=PyPDFLoader,
                show_progress=True
            )
            pdf_docs = pdf_loader.load()
            documents.extend(pdf_docs)
            print(f"    âœ… PDFæ–‡æ¡£: {len(pdf_docs)} ä¸ª")

        # æ–‡æœ¬æ–‡ä»¶
        txt_pattern = os.path.join(data_dir, "**/*.txt")
        if glob.glob(txt_pattern, recursive=True):
            print(f"  ğŸ“ åŠ è½½æ–‡æœ¬æ–‡ä»¶...")
            txt_loader = DirectoryLoader(
                data_dir,
                glob="**/*.txt",
                loader_cls=TextLoader,
                show_progress=True
            )
            txt_docs = txt_loader.load()
            documents.extend(txt_docs)
            print(f"    âœ… æ–‡æœ¬æ–‡æ¡£: {len(txt_docs)} ä¸ª")

        # Markdownæ–‡ä»¶
        md_pattern = os.path.join(data_dir, "**/*.md")
        if glob.glob(md_pattern, recursive=True):
            print(f"  ğŸ“‹ åŠ è½½Markdownæ–‡ä»¶...")
            md_loader = DirectoryLoader(
                data_dir,
                glob="**/*.md",
                loader_cls=TextLoader,
                show_progress=True
            )
            md_docs = md_loader.load()
            documents.extend(md_docs)
            print(f"    âœ… Markdownæ–‡æ¡£: {len(md_docs)} ä¸ª")

    except ImportError:
        print(f"  âš ï¸  langchain_communityæœªå®‰è£…ï¼Œè·³è¿‡å…¶ä»–æ ¼å¼")
    except Exception as e:
        print(f"  âŒ å…¶ä»–æ ¼å¼åŠ è½½å¤±è´¥: {e}")

    return documents

def _create_test_documents() -> List[Document]:
    """
    åˆ›å»ºæµ‹è¯•æ–‡æ¡£

    Returns:
        æµ‹è¯•æ–‡æ¡£åˆ—è¡¨
    """
    test_docs = [
        Document(
            page_content="""
ã€ä½œå“åŸºæœ¬ä¿¡æ¯ã€‘

ä½œå“åç§°ï¼šAdaptive Helix ä»¿ç”Ÿè •åŠ¨æœºæ¢°è®¾è®¡
å±•åŒºä½ç½®ï¼šè‰ºæœ¯ä¸ç§‘æŠ€å±•åŒº130509T-X - X01
ä½œå“ç±»åˆ«ï¼šè‰ºæœ¯ä¸ç§‘æŠ€ / å±•ç¤ºè‰ºæœ¯ä¸æŠ€æœ¯
å‘ˆç°å½¢å¼ï¼šæ–‡å­— + å›¾ç‰‡

è®¾è®¡ä½œè€…ï¼šéƒ­æµ·åªš
æŒ‡å¯¼è€å¸ˆï¼šå€ªæ€æ…§
åˆ›ä½œæ—¶é—´ï¼š2025å¹´

ã€ä½œå“ç®€ä»‹ã€‘
åŸºäºç†æŸ¥å¾·ãƒ»é“é‡‘æ–¯ã€Šè‡ªç§çš„åŸºå› ã€‹ç†è®ºçš„è·¨å­¦ç§‘äº’åŠ¨è£…ç½®ï¼Œé€šè¿‡ä»¿ç”Ÿè •åŠ¨æœºæ¢°æ¨¡æ‹Ÿèš¯èš“åœ¨çƒ­å¸¦é›¨æ—ã€æ²™æ¼ å’Œé›ªåœ°ä¸‰ç§ç¯å¢ƒä¸­çš„è¡Œä¸ºã€‚
""",
            metadata={
                "type": "basic_info",
                "category": "è‰ºæœ¯ä¸ç§‘æŠ€",
                "item_name": "Adaptive Helix ä»¿ç”Ÿè •åŠ¨æœºæ¢°è®¾è®¡",
                "source": "test_data"
            }
        ),
        Document(
            page_content="""
ã€ä½œå“åŸºæœ¬ä¿¡æ¯ã€‘

ä½œå“åç§°ï¼šè¯‘æ–‡äº¤äº’ç•Œé¢è®¾è®¡
å±•åŒºä½ç½®ï¼šè‰ºæœ¯ä¸ç§‘æŠ€å±•åŒº130509T-X - X02
ä½œå“ç±»åˆ«ï¼šè‰ºæœ¯ä¸ç§‘æŠ€ / æ•°å­—æ–‡å¨±è®¾è®¡
å‘ˆç°å½¢å¼ï¼šæ–‡å­— + å›¾ç‰‡

è®¾è®¡ä½œè€…ï¼šé’Ÿç‡•è¥
æŒ‡å¯¼è€å¸ˆï¼šç‹å¿ƒå¦
åˆ›ä½œæ—¶é—´ï¼š2025å¹´

ã€ä½œå“ç®€ä»‹ã€‘
ä»¥ä¸­å›½ä¼ ç»Ÿçº¹æ ·ä½œä¸ºç§‘æ™®å†…å®¹çš„äº’åŠ¨ç½‘ç«™ï¼ŒåŒ…å«ä¸ƒå¤§åŠŸèƒ½æ¨¡å—ï¼Œä¸ºç”¨æˆ·æä¾›ä¼ ç»Ÿçº¹æ ·ç°ä»£åŒ–åº”ç”¨çš„è·¯å¾„ã€‚
""",
            metadata={
                "type": "basic_info",
                "category": "è‰ºæœ¯ä¸ç§‘æŠ€",
                "item_name": "è¯‘æ–‡äº¤äº’ç•Œé¢è®¾è®¡",
                "source": "test_data"
            }
        )
    ]

    return test_docs

def _create_system_summary(documents: List[Document], data_dir: str) -> List[Document]:
    """
    åˆ›å»ºç³»ç»Ÿæ‘˜è¦æ–‡æ¡£

    Args:
        documents: æ‰€æœ‰æ–‡æ¡£
        data_dir: æ•°æ®ç›®å½•

    Returns:
        æ‘˜è¦æ–‡æ¡£åˆ—è¡¨
    """
    summary_docs = []

    try:
        # 1. ç³»ç»Ÿä¿¡æ¯æ‘˜è¦
        total_docs = len(documents)

        # ç»Ÿè®¡ä¿¡æ¯
        doc_types = {}
        categories = {}
        sources = set()

        for doc in documents:
            doc_type = doc.metadata.get("type", "unknown")
            category = doc.metadata.get("category", "æœªçŸ¥")
            source = doc.metadata.get("source", "æœªçŸ¥")

            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
            categories[category] = categories.get(category, 0) + 1
            sources.add(os.path.basename(str(source)))

        # ç³»ç»Ÿæ‘˜è¦æ–‡æ¡£
        sys_content = f"""
ã€ç³»ç»Ÿä¿¡æ¯æ‘˜è¦ã€‘

æ•°æ®å…¥åº“æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ•°æ®ç›®å½•ï¼š{os.path.abspath(data_dir)}

ã€æ•°æ®ç»Ÿè®¡ã€‘
æ–‡æ¡£æ€»æ•°ï¼š{total_docs} ä¸ªç‰‡æ®µ
æ•°æ®æºæ–‡ä»¶ï¼š{len(sources)} ä¸ªæ–‡ä»¶

ã€æ–‡æ¡£ç±»å‹åˆ†å¸ƒã€‘
"""

        for doc_type, count in doc_types.items():
            percentage = (count / total_docs * 100) if total_docs > 0 else 0
            sys_content += f"- {doc_type}: {count}ä¸ª ({percentage:.1f}%)\n"

        sys_content += f"""
ã€ä½œå“ç±»åˆ«åˆ†å¸ƒã€‘
"""

        for category, count in categories.items():
            if category != "æœªçŸ¥":
                percentage = (count / total_docs * 100) if total_docs > 0 else 0
                sys_content += f"- {category}: {count}ä¸ª ({percentage:.1f}%)\n"

        sys_content += f"""
ã€ç³»ç»Ÿè¯´æ˜ã€‘
æ­¤æ•°æ®åº“åŒ…å«è‰ºæœ¯ä¸ç§‘æŠ€å±•è§ˆä½œå“çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ”¯æŒæŒ‰å±•åŒºã€ç±»åˆ«ã€ä½œè€…ã€æŠ€æœ¯ç­‰ç»´åº¦æ£€ç´¢ã€‚
"""

        sys_doc = Document(
            page_content=sys_content.strip(),
            metadata={
                "source": "ingest.py",
                "type": "system_summary",
                "created_at": datetime.now().isoformat(),
                "total_documents": total_docs,
                "data_dir": data_dir
            }
        )
        summary_docs.append(sys_doc)

        # 2. ä½¿ç”¨æŒ‡å—æ–‡æ¡£
        guide_content = f"""
ã€å±•è§ˆæ•°æ®åº“ä½¿ç”¨æŒ‡å—ã€‘

ğŸ“Œ æŸ¥è¯¢ç¤ºä¾‹ï¼š
1. æŒ‰å±•åŒºæŸ¥è¯¢ï¼šæœç´¢"è‰ºæœ¯ä¸ç§‘æŠ€å±•åŒº130509T-X"
2. æŒ‰ç±»åˆ«æŸ¥è¯¢ï¼šæœç´¢"æ•°å­—æ–‡å¨±è®¾è®¡"æˆ–"è‰ºæœ¯ä¸ç§‘æŠ€"
3. æŒ‰ä½œè€…æŸ¥è¯¢ï¼šæœç´¢"éƒ­æµ·åªš"æˆ–"è®¾è®¡ä½œè€…"
4. æŒ‰æŠ€æœ¯æŸ¥è¯¢ï¼šæœç´¢"RFID"ã€"äº’åŠ¨è£…ç½®"ã€"3Då»ºæ¨¡"
5. æŒ‰ä½œå“æŸ¥è¯¢ï¼šæœç´¢"Adaptive Helix"æˆ–"è¯‘æ–‡äº¤äº’"

ğŸ“Œ æ£€ç´¢æŠ€å·§ï¼š
- ä½¿ç”¨å…·ä½“å…³é”®è¯è·å¾—æ›´ç²¾ç¡®ç»“æœ
- å¯ä»¥ç»„åˆæŸ¥è¯¢ï¼šå¦‚"è‰ºæœ¯ä¸ç§‘æŠ€ äº’åŠ¨è£…ç½®"
- ç³»ç»Ÿæ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢

ğŸ“Œ æ•°æ®å†…å®¹ï¼š
- ä½œå“åŸºæœ¬ä¿¡æ¯ï¼ˆåç§°ã€ä½œè€…ã€æ—¶é—´ç­‰ï¼‰
- è¯¦ç»†è®¾è®¡æè¿°ï¼ˆç†å¿µã€æŠ€æœ¯ã€æ•ˆæœç­‰ï¼‰
- å›¾ç‰‡ä¿¡æ¯ï¼ˆæ–‡ä»¶è·¯å¾„ã€è¯´æ˜ç­‰ï¼‰

ğŸ“Œ ç³»ç»Ÿç‰¹æ€§ï¼š
- æ”¯æŒå¤šsheet Excelæ–‡ä»¶å¤„ç†
- æ™ºèƒ½æ–‡æ¡£åˆ‡åˆ†å’Œå‘é‡åŒ–
- åŸºäºè¯­ä¹‰çš„ç›¸ä¼¼åº¦æ£€ç´¢
"""

        guide_doc = Document(
            page_content=guide_content.strip(),
            metadata={
                "source": "ingest.py",
                "type": "user_guide",
                "version": "1.0",
                "created_at": datetime.now().isoformat()
            }
        )
        summary_docs.append(guide_doc)

    except Exception as e:
        print(f"  âŒ åˆ›å»ºæ‘˜è¦å¤±è´¥: {e}")

    return summary_docs

def split_documents(documents: List[Document]) -> List[Document]:
    """
    æ™ºèƒ½æ–‡æ¡£åˆ‡åˆ†

    Args:
        documents: è¾“å…¥æ–‡æ¡£åˆ—è¡¨

    Returns:
        åˆ‡åˆ†åçš„æ–‡æ¡£åˆ—è¡¨
    """
    if not documents:
        return []

    print(f"\nâœ‚ï¸  æ–‡æ¡£æ™ºèƒ½åˆ‡åˆ†")
    print(f"è¾“å…¥æ–‡æ¡£: {len(documents)} ä¸ª")

    chunks = []

    # åˆ†ç¦»ä¸åŒç±»å‹çš„æ–‡æ¡£
    basic_info_docs = []
    detailed_info_docs = []
    image_info_docs = []
    concept_docs = []
    summary_docs = []
    other_docs = []

    for doc in documents:
        doc_type = doc.metadata.get("type", "unknown")

        if doc_type == "basic_info":
            basic_info_docs.append(doc)
        elif doc_type == "detailed_info":
            detailed_info_docs.append(doc)
        elif doc_type == "image_info":
            image_info_docs.append(doc)
        elif doc_type == "design_concept":
            concept_docs.append(doc)
        elif doc_type in ["system_summary", "user_guide"]:
            summary_docs.append(doc)
        else:
            other_docs.append(doc)

    print(f"ğŸ“‹ æ–‡æ¡£ç±»å‹ç»Ÿè®¡:")
    print(f"  åŸºæœ¬ä¿¡æ¯: {len(basic_info_docs)}")
    print(f"  è¯¦ç»†æè¿°: {len(detailed_info_docs)}")
    print(f"  å›¾ç‰‡ä¿¡æ¯: {len(image_info_docs)}")
    print(f"  è®¾è®¡ç†å¿µ: {len(concept_docs)}")
    print(f"  ç³»ç»Ÿæ‘˜è¦: {len(summary_docs)}")
    print(f"  å…¶ä»–æ–‡æ¡£: {len(other_docs)}")

    # 1. åŸºæœ¬ä¿¡æ¯æ–‡æ¡£ï¼šä¸åˆ‡åˆ†ï¼ˆé€šå¸¸è¾ƒçŸ­ï¼‰
    chunks.extend(basic_info_docs)
    print(f"âœ… åŸºæœ¬ä¿¡æ¯æ–‡æ¡£: ä¿æŒåŸæ ·")

    # 2. å›¾ç‰‡ä¿¡æ¯æ–‡æ¡£ï¼šä¸åˆ‡åˆ†
    chunks.extend(image_info_docs)
    print(f"âœ… å›¾ç‰‡ä¿¡æ¯æ–‡æ¡£: ä¿æŒåŸæ ·")

    # 3. è¯¦ç»†æè¿°æ–‡æ¡£ï¼šéœ€è¦åˆ‡åˆ†ï¼ˆå¯èƒ½è¾ƒé•¿ï¼‰
    if detailed_info_docs:
        detailed_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""],
            length_function=len,
        )
        detailed_chunks = detailed_splitter.split_documents(detailed_info_docs)
        chunks.extend(detailed_chunks)
        print(f"âœ… è¯¦ç»†æè¿°æ–‡æ¡£: {len(detailed_info_docs)} â†’ {len(detailed_chunks)}")

    # 4. è®¾è®¡ç†å¿µæ–‡æ¡£ï¼šé€‚å½“åˆ‡åˆ†
    if concept_docs:
        concept_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=80,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""],
            length_function=len,
        )
        concept_chunks = concept_splitter.split_documents(concept_docs)
        chunks.extend(concept_chunks)
        print(f"âœ… è®¾è®¡ç†å¿µæ–‡æ¡£: {len(concept_docs)} â†’ {len(concept_chunks)}")

    # 5. æ‘˜è¦æ–‡æ¡£ï¼šé€‚å½“åˆ‡åˆ†
    if summary_docs:
        summary_splitter = RecursiveCharacterTextSplitter(
            chunk_size=700,
            chunk_overlap=90,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""],
            length_function=len,
        )
        summary_chunks = summary_splitter.split_documents(summary_docs)
        chunks.extend(summary_chunks)
        print(f"âœ… ç³»ç»Ÿæ‘˜è¦æ–‡æ¡£: {len(summary_docs)} â†’ {len(summary_chunks)}")

    # 6. å…¶ä»–æ–‡æ¡£ï¼šé»˜è®¤åˆ‡åˆ†
    if other_docs:
        default_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""],
            length_function=len,
        )
        other_chunks = default_splitter.split_documents(other_docs)
        chunks.extend(other_chunks)
        print(f"âœ… å…¶ä»–æ–‡æ¡£: {len(other_docs)} â†’ {len(other_chunks)}")

    print(f"ğŸ“ˆ åˆ‡åˆ†å®Œæˆ: {len(documents)} â†’ {len(chunks)} ä¸ªç‰‡æ®µ")

    return chunks

def build_vector_database(data_dir: str = "data/raw_docs") -> Tuple[bool, Dict[str, Any]]:
    """
    æ„å»ºå‘é‡æ•°æ®åº“

    Args:
        data_dir: æ•°æ®ç›®å½•

    Returns:
        (æˆåŠŸæ ‡å¿—, ç»Ÿè®¡ä¿¡æ¯)
    """
    print("\n" + "=" * 70)
    print("ğŸ—ï¸  å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“")
    print("=" * 70)

    stats = {
        "status": "started",
        "start_time": datetime.now().isoformat(),
        "data_dir": data_dir
    }

    try:
        # æ£€æŸ¥ä¾èµ–
        if not EMBEDDINGS_AVAILABLE:
            print("âŒ åµŒå…¥æ¨¡å—ä¸å¯ç”¨")
            stats["status"] = "error"
            stats["error"] = "Embeddings module not available"
            return False, stats

        # 1. åŠ è½½æ–‡æ¡£
        print(f"\n1ï¸âƒ£ åŠ è½½æ–‡æ¡£...")
        documents = load_documents(data_dir)

        if not documents:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°æ–‡æ¡£")
            stats["status"] = "error"
            stats["error"] = "No documents loaded"
            return False, stats

        stats["loaded_documents"] = len(documents)

        # 2. æ–‡æ¡£åˆ‡åˆ†
        print(f"\n2ï¸âƒ£ æ–‡æ¡£åˆ‡åˆ†...")
        chunks = split_documents(documents)

        if not chunks:
            print("âŒ æ–‡æ¡£åˆ‡åˆ†å¤±è´¥")
            stats["status"] = "error"
            stats["error"] = "Document splitting failed"
            return False, stats

        stats["chunks_after_splitting"] = len(chunks)

        # 3. å‡†å¤‡æ•°æ®...
        print(f"\n3ï¸âƒ£ å‡†å¤‡æ•°æ®...")
        texts = [chunk.page_content for chunk in chunks]
        metadatas = []

        def sanitize_metadata(metadata):
            """æ¸…ç†å…ƒæ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ChromaDBæ”¯æŒçš„ç±»å‹"""
            sanitized = {}
            for key, value in metadata.items():
                if value is None:
                    sanitized[key] = None
                elif isinstance(value, (str, int, float, bool)):
                    sanitized[key] = value
                elif isinstance(value, list):
                    # åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                    sanitized[key] = ", ".join(str(item) for item in value)
                elif isinstance(value, dict):
                    # å­—å…¸è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                    import json
                    sanitized[key] = json.dumps(value, ensure_ascii=False)
                elif isinstance(value, set):
                    # é›†åˆè½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                    sanitized[key] = ", ".join(str(item) for item in value)
                else:
                    # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    sanitized[key] = str(value)
            return sanitized

        for i, chunk in enumerate(chunks):
            metadata = chunk.metadata.copy()

            # æ¸…ç†å…ƒæ•°æ®
            metadata = sanitize_metadata(metadata)

            metadata["chunk_id"] = i
            metadata["chunk_length"] = len(chunk.page_content)
            metadata["ingest_time"] = datetime.now().isoformat()
            metadatas.append(metadata)

        print(f"  æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"  å¹³å‡é•¿åº¦: {sum(len(t) for t in texts) // len(texts) if texts else 0} å­—ç¬¦")

        stats["texts_prepared"] = len(texts)
        stats["avg_text_length"] = sum(len(t) for t in texts) // len(texts) if texts else 0

        # 4. å‘é‡åŒ–
        print(f"\n4ï¸âƒ£ æ–‡æœ¬å‘é‡åŒ–...")
        embedder = get_embedding_manager()
        embeddings = embedder.embed_texts(texts)

        if not embeddings:
            print("âŒ å‘é‡åŒ–å¤±è´¥")
            stats["status"] = "error"
            stats["error"] = "Text embedding failed"
            return False, stats

        stats["embeddings_created"] = len(embeddings)
        stats["embedding_dimension"] = len(embeddings[0]) if embeddings else 0

        # 5. å­˜å…¥å‘é‡æ•°æ®åº“
        print(f"\n5ï¸âƒ£ ä¿å­˜åˆ°å‘é‡æ•°æ®åº“...")
        collection = embedder.get_or_create_collection()

        # ç”Ÿæˆå”¯ä¸€ID
        ids = [f"doc_{uuid.uuid4().hex[:12]}" for _ in range(len(texts))]

        # æ‰¹é‡æ·»åŠ ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        batch_size = 100
        total_added = 0

        for i in range(0, len(texts), batch_size):
            end_idx = min(i + batch_size, len(texts))

            batch_texts = texts[i:end_idx]
            batch_embeddings = embeddings[i:end_idx]
            batch_metadatas = metadatas[i:end_idx]
            batch_ids = ids[i:end_idx]

            collection.add(
                embeddings=batch_embeddings,
                documents=batch_texts,
                metadatas=batch_metadatas,
                ids=batch_ids
            )

            total_added += len(batch_texts)
            progress = (total_added / len(texts)) * 100
            print(f"  è¿›åº¦: {total_added}/{len(texts)} ({progress:.1f}%)")

        # 6. æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        collection_count = collection.count()

        # æ–‡æ¡£ç±»å‹ç»Ÿè®¡
        doc_types = {}
        categories = {}

        for metadata in metadatas:
            doc_type = metadata.get("type", "unknown")
            category = metadata.get("category", "æœªçŸ¥")

            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
            if category != "æœªçŸ¥":
                categories[category] = categories.get(category, 0) + 1

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats.update({
            "status": "success",
            "end_time": datetime.now().isoformat(),
            "collection_name": collection.name,
            "collection_count": collection_count,
            "document_types": doc_types,
            "categories": categories,
            "total_processing_time": (
                datetime.now() - datetime.fromisoformat(stats["start_time"])
            ).total_seconds()
        })

        # 7. æ˜¾ç¤ºç»“æœ
        print(f"\n" + "=" * 70)
        print("ğŸ‰ å‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸï¼")
        print("=" * 70)

        print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        print(f"  é›†åˆåç§°: {collection.name}")
        print(f"  å­˜å‚¨ä½ç½®: data/chroma_db/")
        print(f"  æ€»æ–‡æ¡£æ•°: {collection_count}")
        print(f"  å‘é‡ç»´åº¦: {len(embeddings[0]) if embeddings else 0}")

        print(f"\nğŸ“‹ æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
        for doc_type, count in doc_types.items():
            percentage = (count / collection_count * 100) if collection_count > 0 else 0
            print(f"  {doc_type}: {count} ({percentage:.1f}%)")

        print(f"\nğŸ¨ ä½œå“ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in categories.items():
            if category != "æœªçŸ¥":
                percentage = (count / collection_count * 100) if collection_count > 0 else 0
                print(f"  {category}: {count} ({percentage:.1f}%)")

        print(f"\nâ±ï¸  å¤„ç†æ—¶é—´: {stats['total_processing_time']:.2f} ç§’")

        print(f"\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print(f"  1. æµ‹è¯•æ£€ç´¢: python backend/rag/retriever.py")
        print(f"  2. å¯åŠ¨API: uvicorn backend.app:app --host 0.0.0.0 --port 8000")
        print(f"  3. è®¿é—®Web: http://localhost:8000")

        return True, stats

    except Exception as e:
        print(f"\nâŒ æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        stats.update({
            "status": "error",
            "end_time": datetime.now().isoformat(),
            "error": str(e),
            "total_processing_time": (
                datetime.now() - datetime.fromisoformat(stats["start_time"])
            ).total_seconds()
        })

        return False, stats

def clear_vector_database(collection_name: str = "exhibition_docs") -> bool:
    """
    æ¸…ç©ºå‘é‡æ•°æ®åº“

    Args:
        collection_name: é›†åˆåç§°

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    print(f"\nâš ï¸  è­¦å‘Šï¼šå³å°†æ¸…ç©ºå‘é‡æ•°æ®åº“ï¼")
    print(f"é›†åˆåç§°: {collection_name}")

    confirm = input("è¯·è¾“å…¥ 'DELETE' ç¡®è®¤æ“ä½œ: ")

    if confirm != "DELETE":
        print("æ“ä½œå·²å–æ¶ˆ")
        return False

    try:
        if not EMBEDDINGS_AVAILABLE:
            print("âŒ åµŒå…¥æ¨¡å—ä¸å¯ç”¨")
            return False

        embedder = get_embedding_manager()
        collection = embedder.get_or_create_collection(collection_name)

        # è·å–å½“å‰æ–‡æ¡£æ•°
        current_count = collection.count()

        # åˆ é™¤æ‰€æœ‰æ–‡æ¡£
        collection.delete(where={})

        print(f"âœ… å‘é‡æ•°æ®åº“å·²æ¸…ç©º")
        print(f"  åˆ é™¤æ–‡æ¡£æ•°: {current_count}")
        print(f"  å½“å‰æ–‡æ¡£æ•°: {collection.count()}")

        return True

    except Exception as e:
        print(f"âŒ æ¸…ç©ºå¤±è´¥: {e}")
        return False

def get_database_info() -> Dict[str, Any]:
    """
    è·å–æ•°æ®åº“ä¿¡æ¯

    Returns:
        æ•°æ®åº“ä¿¡æ¯å­—å…¸
    """
    try:
        if not EMBEDDINGS_AVAILABLE:
            return {"status": "error", "error": "Embeddings module not available"}

        embedder = get_embedding_manager()
        info = embedder.get_collection_info()

        return info

    except Exception as e:
        return {"status": "error", "error": str(e)}

if __name__ == "__main__":
    """
    å‘½ä»¤è¡Œæ¥å£
    """
    import argparse

    parser = argparse.ArgumentParser(
        description="è‰ºæœ¯ä¸ç§‘æŠ€å±•è§ˆæ•°æ®RAGå…¥åº“ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python ingest.py --build                    # æ„å»ºå‘é‡æ•°æ®åº“
  python ingest.py --clear                    # æ¸…ç©ºæ•°æ®åº“
  python ingest.py --info                     # æŸ¥çœ‹æ•°æ®åº“ä¿¡æ¯
  python ingest.py --data-dir /path/to/data   # æŒ‡å®šæ•°æ®ç›®å½•
        """
    )

    parser.add_argument("--build", action="store_true", help="æ„å»ºå‘é‡æ•°æ®åº“")
    parser.add_argument("--clear", action="store_true", help="æ¸…ç©ºå‘é‡æ•°æ®åº“")
    parser.add_argument("--info", action="store_true", help="æŸ¥çœ‹æ•°æ®åº“ä¿¡æ¯")
    parser.add_argument("--data-dir", type=str, default="data/raw_docs",
                       help="æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: data/raw_docs)")

    args = parser.parse_args()

    # æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.clear:
        success = clear_vector_database()
        sys.exit(0 if success else 1)

    elif args.info:
        info = get_database_info()
        print("ğŸ“Š æ•°æ®åº“ä¿¡æ¯:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        sys.exit(0)

    elif args.build:
        success, stats = build_vector_database(args.data_dir)
        sys.exit(0 if success else 1)

    else:
        # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©
        parser.print_help()
        print(f"\nğŸ¨ è¯·ä½¿ç”¨ä»¥ä¸Šå‚æ•°è¿è¡Œæœ¬ç¨‹åº")
        sys.exit(1)
