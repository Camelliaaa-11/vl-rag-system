"""
å‘é‡åŒ–åµŒå…¥ç®¡ç†æ¨¡å— - ä½¿ç”¨ transformers ç‰ˆæœ¬
"""
import os
import sys
from typing import List, Dict, Any
import numpy as np
import torch
import chromadb
from chromadb.config import Settings


class EmbeddingManager:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ç®¡ç†å™¨ - ä½¿ç”¨ transformers
        """
        # å®šä¹‰æœ¬åœ°æ¨¡å‹è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(current_dir))
        self.model_dir = os.path.join(project_root, "models", "all-MiniLM-L6-v2")

        print(f"ğŸ”§ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (transformers)")
        print(f"   æ¨¡å‹è·¯å¾„: {self.model_dir}")

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        required_files = ["pytorch_model.bin", "config.json", "tokenizer.json"]
        for file in required_files:
            if not os.path.exists(os.path.join(self.model_dir, file)):
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ç¼ºå¤±: {file}")

        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['TRANSFORMERS_OFFLINE'] = "1"
            os.environ['HF_HUB_OFFLINE'] = "1"

            # å¯¼å…¥ transformers
            from transformers import AutoTokenizer, AutoModel

            print(f"   åŠ è½½ tokenizer å’Œ model...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_dir)
            self.model = AutoModel.from_pretrained(self.model_dir)

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()

            # è·å–æ¨¡å‹ç»´åº¦ï¼ˆä»config.jsonï¼‰
            import json
            config_path = os.path.join(self.model_dir, "config.json")
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            self.embedding_dim = config.get("hidden_size", 384)

            print(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å‘é‡ç»´åº¦: {self.embedding_dim}")
            print(f"   æ¨¡å‹ç±»å‹: {type(self.model).__name__}")

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡æ–‡æœ¬å‘é‡åŒ– - ä½¿ç”¨å‡å€¼æ± åŒ–
        """
        if not texts:
            return []

        print(f"ğŸ”¢ å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æœ¬...")

        try:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜é—®é¢˜
            batch_size = 32
            all_embeddings = []

            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]

                # Tokenize
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=256,
                    return_tensors="pt"
                )

                # æ¨ç†
                with torch.no_grad():
                    outputs = self.model(**inputs)

                # å‡å€¼æ± åŒ–è·å¾—å¥å­å‘é‡
                # attention_mask ç”¨äºå¿½ç•¥padding
                attention_mask = inputs['attention_mask']
                token_embeddings = outputs.last_hidden_state

                # æ‰©å±• attention_mask ä»¥åŒ¹é…åµŒå…¥ç»´åº¦
                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()

                # åŠ æƒå¹³å‡
                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                batch_embeddings = sum_embeddings / sum_mask

                # å½’ä¸€åŒ–ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦éœ€è¦ï¼‰
                batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)

                all_embeddings.append(batch_embeddings.numpy())

            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
            embeddings_array = np.vstack(all_embeddings)
            embeddings_list = embeddings_array.tolist()

            print(f"âœ… å‘é‡åŒ–å®Œæˆ")
            print(f"  å‘é‡ç»´åº¦: {len(embeddings_list[0]) if embeddings_list else 0}")
            print(f"  å‘é‡æ•°é‡: {len(embeddings_list)}")

            return embeddings_list

        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def embed_query(self, query: str) -> List[float]:
        """
        å•ä¸ªæŸ¥è¯¢å‘é‡åŒ–
        """
        return self.embed_texts([query])[0]

    def create_chroma_client(self, persist_directory: str = "data/chroma_db"):
        """
        åˆ›å»ºChromaDBå®¢æˆ·ç«¯
        """
        os.makedirs(persist_directory, exist_ok=True)

        client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True,
                is_persistent=True
            )
        )

        return client

    def get_or_create_collection(self,
                                 collection_name: str = "exhibition_docs",
                                 persist_directory: str = "data/chroma_db"):
        """
        è·å–æˆ–åˆ›å»ºå‘é‡é›†åˆ
        """
        client = self.create_chroma_client(persist_directory)

        try:
            collection = client.get_collection(name=collection_name)
            print(f"ğŸ“‚ åŠ è½½ç°æœ‰é›†åˆ: {collection_name}")
            print(f"  æ–‡æ¡£æ•°é‡: {collection.count()}")

        except Exception:
            collection = client.create_collection(
                name=collection_name,
                metadata={
                    "hnsw:space": "cosine",
                    "description": "è‰ºæœ¯ä¸ç§‘æŠ€å±•è§ˆä½œå“æ•°æ®åº“",
                    "created_by": "RAG System",
                    "embedding_model": "all-MiniLM-L6-v2 (transformers)",
                    "embedding_dim": self.embedding_dim
                }
            )
            print(f"ğŸ“‚ åˆ›å»ºæ–°é›†åˆ: {collection_name}")

        return collection

    def get_collection_info(self, collection_name: str = "exhibition_docs") -> Dict[str, Any]:
        """
        è·å–é›†åˆä¿¡æ¯
        """
        try:
            collection = self.get_or_create_collection(collection_name)

            info = {
                "collection_name": collection.name,
                "document_count": collection.count(),
                "metadata": collection.metadata,
                "embedding_dim": self.embedding_dim,
                "status": "active"
            }

            return info

        except Exception as e:
            return {
                "collection_name": collection_name,
                "error": str(e),
                "status": "error"
            }


# å…¨å±€å•ä¾‹å®ä¾‹
_embedding_manager = None


def get_embedding_manager(model_name: str = "all-MiniLM-L6-v2") -> EmbeddingManager:
    """
    è·å–åµŒå…¥ç®¡ç†å™¨å•ä¾‹
    """
    global _embedding_manager

    if _embedding_manager is None:
        _embedding_manager = EmbeddingManager(model_name)

    return _embedding_manager


if __name__ == "__main__":
    # æ¨¡å—æµ‹è¯•
    print("ğŸ§ª embeddings.py æ¨¡å—æµ‹è¯• (transformersç‰ˆ)")
    print("=" * 50)

    try:
        manager = get_embedding_manager()

        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯å‘é‡åŒ–åŠŸèƒ½ã€‚",
            "è‰ºæœ¯ä¸ç§‘æŠ€å±•è§ˆä½œå“æ•°æ®åº“",
            "æ•°å­—æ–‡å¨±è®¾è®¡ï¼Œäº’åŠ¨è£…ç½®ï¼Œåˆ›æ–°æŠ€æœ¯"
        ]

        embeddings = manager.embed_texts(test_texts)

        print(f"âœ… æµ‹è¯•é€šè¿‡")
        print(f"  æµ‹è¯•æ–‡æœ¬æ•°é‡: {len(test_texts)}")
        print(f"  ç”Ÿæˆå‘é‡æ•°é‡: {len(embeddings)}")
        print(f"  å‘é‡ç»´åº¦: {len(embeddings[0]) if embeddings else 0}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
